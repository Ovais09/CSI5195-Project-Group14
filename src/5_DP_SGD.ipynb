{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from opacus import PrivacyEngine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ovaze\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\opacus\\privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ovaze\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ovaze\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4622\n",
      "Epoch [2/10], Loss: 0.5026\n",
      "Epoch [3/10], Loss: 0.4172\n",
      "Epoch [4/10], Loss: 0.3912\n",
      "Epoch [5/10], Loss: 0.4927\n",
      "Epoch [6/10], Loss: 0.3366\n",
      "Epoch [7/10], Loss: 0.3545\n",
      "Epoch [8/10], Loss: 0.5549\n",
      "Epoch [9/10], Loss: 0.3957\n",
      "Epoch [10/10], Loss: 0.3845\n",
      "\n",
      " DP-SGD Test Accuracy: 0.8167\n",
      "\n",
      " DP-SGD Model & Scaler Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv(\"../Data/The_Cancer_data_DP_Protected.csv\")\n",
    "\n",
    "# Split Features & Target Variable\n",
    "X = df.drop(columns=[\"Diagnosis\"]).values  # Convert to NumPy array\n",
    "y = df[\"Diagnosis\"].values\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for DP-SGD\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define Logistic Regression Model in PyTorch\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 2)  # 2 outputs (Cancer Positive/Negative)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LogisticRegressionModel(input_size=X_train.shape[1])\n",
    "\n",
    "# Define Loss Function & Optimizer (DP-SGD)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Apply Differential Privacy Using Opacus\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    epochs=10,\n",
    "    target_epsilon=3.0,  # Privacy Budget\n",
    "    target_delta=1e-5,\n",
    "    max_grad_norm=1.0,  # Clipping Gradient Norm\n",
    ")\n",
    "\n",
    "# Train the DP-SGD Model\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate Model on Test Data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_tensor).argmax(dim=1).numpy()\n",
    "\n",
    "# Calculate Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\n DP-SGD Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Save Model & Scaler\n",
    "torch.save(model.state_dict(), \"../dp_sgd_model.pth\")\n",
    "torch.save(scaler, \"../scaler_SGD.pkl\")\n",
    "print(\"\\n DP-SGD Model & Scaler Saved Successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
